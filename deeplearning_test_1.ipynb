{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f0246dc-3686-4f69-a5bb-0921ad496ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Prediction: 0\n",
      "Input: [0 1], Prediction: 0\n",
      "Input: [1 0], Prediction: 0\n",
      "Input: [1 1], Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Initialize the Perceptron class\n",
    "class Perceptron:\n",
    "    def __init__(self, input_dim, learning_rate=0.1):\n",
    "        self.weights = np.random.randn(input_dim)  # Random initialization of weights\n",
    "        self.bias = np.random.randn(1)  # Random initialization of bias\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    # Step 2: Activation function (Step Function)\n",
    "    def activation(self, x):\n",
    "        return 1 if x >= 0 else 0\n",
    "\n",
    "    # Step 3: Training method\n",
    "    def train(self, X, y, epochs=100):\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(X)):\n",
    "                # Weighted sum\n",
    "                linear_output = np.dot(X[i], self.weights) + self.bias\n",
    "                # Apply the activation function\n",
    "                prediction = self.activation(linear_output)\n",
    "                # Update weights and bias if the prediction is incorrect\n",
    "                error = y[i] - prediction\n",
    "                self.weights += self.learning_rate * error * X[i]\n",
    "                self.bias += self.learning_rate * error\n",
    "\n",
    "    # Step 4: Prediction method\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return self.activation(linear_output)\n",
    "\n",
    "# Step 5: Example usage\n",
    "\n",
    "# Training data (AND logic gate example)\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "y = np.array([0, 0, 0, 1])  # Labels\n",
    "\n",
    "# Initialize perceptron with 2 inputs (AND gate has 2 inputs)\n",
    "perceptron = Perceptron(input_dim=2)\n",
    "\n",
    "# Train the perceptron\n",
    "perceptron.train(X, y, epochs=100)\n",
    "\n",
    "# Test predictions\n",
    "for x in X:\n",
    "    print(f\"Input: {x}, Prediction: {perceptron.predict(x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626e05cb-ca8b-493e-ba70-7843ec4765b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # features\n",
    "y = data.target  # labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Encode the labels (One-hot encoding for multi-class)\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765171cc-0c18-4a76-87e4-4f034428a624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 140ms/step - accuracy: 0.2123 - loss: 1.2511 - val_accuracy: 0.1667 - val_loss: 1.2761\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2223 - loss: 1.2001 - val_accuracy: 0.1667 - val_loss: 1.2361\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2285 - loss: 1.1521 - val_accuracy: 0.1667 - val_loss: 1.1972\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2196 - loss: 1.1391 - val_accuracy: 0.1667 - val_loss: 1.1590\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.2523 - loss: 1.0819 - val_accuracy: 0.2000 - val_loss: 1.1224\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2800 - loss: 1.0725 - val_accuracy: 0.2000 - val_loss: 1.0868\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2879 - loss: 1.0288 - val_accuracy: 0.2333 - val_loss: 1.0526\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3444 - loss: 0.9985 - val_accuracy: 0.3000 - val_loss: 1.0205\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4802 - loss: 0.9598 - val_accuracy: 0.3667 - val_loss: 0.9900\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5085 - loss: 0.9403 - val_accuracy: 0.3667 - val_loss: 0.9603\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4958 - loss: 0.9260 - val_accuracy: 0.4000 - val_loss: 0.9327\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5104 - loss: 0.9056 - val_accuracy: 0.4333 - val_loss: 0.9072\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5481 - loss: 0.8555 - val_accuracy: 0.4667 - val_loss: 0.8831\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5615 - loss: 0.8607 - val_accuracy: 0.5000 - val_loss: 0.8596\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5427 - loss: 0.8496 - val_accuracy: 0.5333 - val_loss: 0.8371\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5788 - loss: 0.8091 - val_accuracy: 0.5333 - val_loss: 0.8156\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5748 - loss: 0.8075 - val_accuracy: 0.5667 - val_loss: 0.7957\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6190 - loss: 0.7721 - val_accuracy: 0.6000 - val_loss: 0.7768\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6525 - loss: 0.7522 - val_accuracy: 0.6333 - val_loss: 0.7587\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6223 - loss: 0.7519 - val_accuracy: 0.6333 - val_loss: 0.7415\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6377 - loss: 0.7152 - val_accuracy: 0.6667 - val_loss: 0.7254\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6575 - loss: 0.7256 - val_accuracy: 0.7000 - val_loss: 0.7098\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6575 - loss: 0.7043 - val_accuracy: 0.7000 - val_loss: 0.6949\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6369 - loss: 0.7152 - val_accuracy: 0.7000 - val_loss: 0.6809\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7035 - loss: 0.6465 - val_accuracy: 0.7000 - val_loss: 0.6673\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6998 - loss: 0.6742 - val_accuracy: 0.6667 - val_loss: 0.6549\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7142 - loss: 0.6338 - val_accuracy: 0.7000 - val_loss: 0.6426\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.6471 - loss: 0.6681 - val_accuracy: 0.7000 - val_loss: 0.6311\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.6567 - loss: 0.6468 - val_accuracy: 0.7000 - val_loss: 0.6199\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7260 - loss: 0.6050 - val_accuracy: 0.7000 - val_loss: 0.6091\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7275 - loss: 0.5973 - val_accuracy: 0.7000 - val_loss: 0.5985\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7121 - loss: 0.6234 - val_accuracy: 0.7000 - val_loss: 0.5884\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.7110 - loss: 0.6072 - val_accuracy: 0.7000 - val_loss: 0.5788\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7423 - loss: 0.5842 - val_accuracy: 0.7333 - val_loss: 0.5695\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7623 - loss: 0.5489 - val_accuracy: 0.7333 - val_loss: 0.5605\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7477 - loss: 0.5510 - val_accuracy: 0.7333 - val_loss: 0.5519\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7754 - loss: 0.5443 - val_accuracy: 0.7333 - val_loss: 0.5436\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7075 - loss: 0.5777 - val_accuracy: 0.7333 - val_loss: 0.5359\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7442 - loss: 0.5464 - val_accuracy: 0.7333 - val_loss: 0.5283\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7265 - loss: 0.5232 - val_accuracy: 0.7667 - val_loss: 0.5211\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7692 - loss: 0.5078 - val_accuracy: 0.7667 - val_loss: 0.5141\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7327 - loss: 0.5294 - val_accuracy: 0.7667 - val_loss: 0.5074\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7544 - loss: 0.5211 - val_accuracy: 0.7667 - val_loss: 0.5008\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7429 - loss: 0.5042 - val_accuracy: 0.7667 - val_loss: 0.4945\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7512 - loss: 0.5027 - val_accuracy: 0.7667 - val_loss: 0.4882\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.6898 - loss: 0.5346 - val_accuracy: 0.7667 - val_loss: 0.4823\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7398 - loss: 0.4901 - val_accuracy: 0.7667 - val_loss: 0.4765\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7379 - loss: 0.4968 - val_accuracy: 0.7667 - val_loss: 0.4708\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7369 - loss: 0.4904 - val_accuracy: 0.7667 - val_loss: 0.4652\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7371 - loss: 0.4755 - val_accuracy: 0.7667 - val_loss: 0.4598\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7352 - loss: 0.4777 - val_accuracy: 0.7667 - val_loss: 0.4546\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7635 - loss: 0.4786 - val_accuracy: 0.7667 - val_loss: 0.4494\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8012 - loss: 0.4444 - val_accuracy: 0.7667 - val_loss: 0.4443\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7462 - loss: 0.4889 - val_accuracy: 0.7667 - val_loss: 0.4396\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.7696 - loss: 0.4673 - val_accuracy: 0.8000 - val_loss: 0.4348\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7790 - loss: 0.4595 - val_accuracy: 0.8000 - val_loss: 0.4303\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7987 - loss: 0.4452 - val_accuracy: 0.8000 - val_loss: 0.4257\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7852 - loss: 0.4555 - val_accuracy: 0.8000 - val_loss: 0.4213\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7738 - loss: 0.4887 - val_accuracy: 0.8000 - val_loss: 0.4169\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7800 - loss: 0.4556 - val_accuracy: 0.8000 - val_loss: 0.4128\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7667 - loss: 0.4492 - val_accuracy: 0.8000 - val_loss: 0.4086\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7752 - loss: 0.4634 - val_accuracy: 0.8000 - val_loss: 0.4045\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7942 - loss: 0.4415 - val_accuracy: 0.8333 - val_loss: 0.4006\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8217 - loss: 0.4432 - val_accuracy: 0.8333 - val_loss: 0.3967\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7852 - loss: 0.4592 - val_accuracy: 0.8333 - val_loss: 0.3929\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8208 - loss: 0.4373 - val_accuracy: 0.8333 - val_loss: 0.3892\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.8198 - loss: 0.4434 - val_accuracy: 0.8333 - val_loss: 0.3856\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8344 - loss: 0.4341 - val_accuracy: 0.8667 - val_loss: 0.3820\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8510 - loss: 0.4356 - val_accuracy: 0.8667 - val_loss: 0.3785\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8365 - loss: 0.4090 - val_accuracy: 0.8667 - val_loss: 0.3751\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8208 - loss: 0.4315 - val_accuracy: 0.8667 - val_loss: 0.3718\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8198 - loss: 0.4136 - val_accuracy: 0.8667 - val_loss: 0.3684\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8208 - loss: 0.4277 - val_accuracy: 0.8667 - val_loss: 0.3651\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8510 - loss: 0.3993 - val_accuracy: 0.8667 - val_loss: 0.3618\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8419 - loss: 0.3857 - val_accuracy: 0.9000 - val_loss: 0.3587\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8450 - loss: 0.4021 - val_accuracy: 0.9000 - val_loss: 0.3556\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8127 - loss: 0.3945 - val_accuracy: 0.9000 - val_loss: 0.3526\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8346 - loss: 0.3696 - val_accuracy: 0.9000 - val_loss: 0.3496\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.8554 - loss: 0.3740 - val_accuracy: 0.9000 - val_loss: 0.3466\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8577 - loss: 0.3782 - val_accuracy: 0.9000 - val_loss: 0.3436\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8525 - loss: 0.3758 - val_accuracy: 0.9000 - val_loss: 0.3407\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8169 - loss: 0.3961 - val_accuracy: 0.9000 - val_loss: 0.3378\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8502 - loss: 0.3888 - val_accuracy: 0.9000 - val_loss: 0.3350\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8460 - loss: 0.3768 - val_accuracy: 0.9000 - val_loss: 0.3323\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8429 - loss: 0.3798 - val_accuracy: 0.9000 - val_loss: 0.3296\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8731 - loss: 0.3547 - val_accuracy: 0.9000 - val_loss: 0.3269\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8679 - loss: 0.3418 - val_accuracy: 0.9000 - val_loss: 0.3243\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8094 - loss: 0.3842 - val_accuracy: 0.9000 - val_loss: 0.3216\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8492 - loss: 0.3523 - val_accuracy: 0.9000 - val_loss: 0.3189\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8294 - loss: 0.3570 - val_accuracy: 0.9000 - val_loss: 0.3162\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8502 - loss: 0.3426 - val_accuracy: 0.9000 - val_loss: 0.3136\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8283 - loss: 0.3663 - val_accuracy: 0.9000 - val_loss: 0.3111\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.8523 - loss: 0.3562 - val_accuracy: 0.9000 - val_loss: 0.3085\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8200 - loss: 0.3550 - val_accuracy: 0.9000 - val_loss: 0.3060\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8367 - loss: 0.3680 - val_accuracy: 0.9000 - val_loss: 0.3034\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8346 - loss: 0.3507 - val_accuracy: 0.9000 - val_loss: 0.3009\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8575 - loss: 0.3284 - val_accuracy: 0.9000 - val_loss: 0.2984\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8575 - loss: 0.3275 - val_accuracy: 0.9000 - val_loss: 0.2960\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8408 - loss: 0.3428 - val_accuracy: 0.9000 - val_loss: 0.2935\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8567 - loss: 0.3314 - val_accuracy: 0.9000 - val_loss: 0.2911\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.9000 - loss: 0.2911\n",
      "Test Loss: 0.2910718321800232, Test Accuracy: 0.8999999761581421\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
      "Predicted labels: [1 0 2 2 2 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "#3.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # features\n",
    "y = data.target  # labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Encode the labels (One-hot encoding for multi-class)\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer + Hidden layer with Xavier (Glorot) initialization\n",
    "model.add(Dense(10, input_dim=4, activation='relu', kernel_initializer=GlorotUniform()))\n",
    "\n",
    "# Output layer (softmax activation for multi-class classification)\n",
    "model.add(Dense(3, activation='softmax', kernel_initializer=GlorotUniform()))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',  # For multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict class labels for the test data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dca590c-8d7f-46c0-bbd5-0517bd14bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 172ms/step - accuracy: 0.7719 - loss: 0.9759 - val_accuracy: 0.8667 - val_loss: 0.9489\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8433 - loss: 0.9429 - val_accuracy: 0.8667 - val_loss: 0.9261\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8204 - loss: 0.9260 - val_accuracy: 0.8667 - val_loss: 0.9034\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8373 - loss: 0.9018 - val_accuracy: 0.9000 - val_loss: 0.8798\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8133 - loss: 0.8870 - val_accuracy: 0.9000 - val_loss: 0.8562\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8321 - loss: 0.8568 - val_accuracy: 0.9000 - val_loss: 0.8319\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8165 - loss: 0.8364 - val_accuracy: 0.9000 - val_loss: 0.8076\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7998 - loss: 0.8209 - val_accuracy: 0.9000 - val_loss: 0.7831\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8183 - loss: 0.7893 - val_accuracy: 0.9000 - val_loss: 0.7585\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8329 - loss: 0.7554 - val_accuracy: 0.9000 - val_loss: 0.7338\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8090 - loss: 0.7448 - val_accuracy: 0.9000 - val_loss: 0.7096\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8163 - loss: 0.7265 - val_accuracy: 0.9000 - val_loss: 0.6856\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8267 - loss: 0.6930 - val_accuracy: 0.9000 - val_loss: 0.6622\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8100 - loss: 0.6827 - val_accuracy: 0.9000 - val_loss: 0.6392\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7998 - loss: 0.6736 - val_accuracy: 0.9000 - val_loss: 0.6169\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8154 - loss: 0.6496 - val_accuracy: 0.9000 - val_loss: 0.5950\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8290 - loss: 0.6223 - val_accuracy: 0.9000 - val_loss: 0.5738\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8206 - loss: 0.6024 - val_accuracy: 0.9000 - val_loss: 0.5535\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8352 - loss: 0.5508 - val_accuracy: 0.9000 - val_loss: 0.5342\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8060 - loss: 0.5648 - val_accuracy: 0.9000 - val_loss: 0.5163\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8383 - loss: 0.5365 - val_accuracy: 0.9000 - val_loss: 0.4990\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8102 - loss: 0.5275 - val_accuracy: 0.9000 - val_loss: 0.4828\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8279 - loss: 0.5253 - val_accuracy: 0.9000 - val_loss: 0.4669\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8290 - loss: 0.4898 - val_accuracy: 0.9000 - val_loss: 0.4520\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7873 - loss: 0.5183 - val_accuracy: 0.9000 - val_loss: 0.4381\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8425 - loss: 0.4621 - val_accuracy: 0.9000 - val_loss: 0.4241\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8427 - loss: 0.4648 - val_accuracy: 0.9000 - val_loss: 0.4107\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8406 - loss: 0.4496 - val_accuracy: 0.9000 - val_loss: 0.3982\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8177 - loss: 0.4407 - val_accuracy: 0.9000 - val_loss: 0.3862\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8156 - loss: 0.4498 - val_accuracy: 0.9000 - val_loss: 0.3745\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8492 - loss: 0.4093 - val_accuracy: 0.9000 - val_loss: 0.3632\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8231 - loss: 0.4208 - val_accuracy: 0.9000 - val_loss: 0.3520\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8304 - loss: 0.3940 - val_accuracy: 0.9000 - val_loss: 0.3413\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8481 - loss: 0.3863 - val_accuracy: 0.9000 - val_loss: 0.3314\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8127 - loss: 0.4058 - val_accuracy: 0.9000 - val_loss: 0.3221\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8338 - loss: 0.3925 - val_accuracy: 0.9000 - val_loss: 0.3127\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8402 - loss: 0.3825 - val_accuracy: 0.9000 - val_loss: 0.3036\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8277 - loss: 0.3811 - val_accuracy: 0.9000 - val_loss: 0.2958\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8610 - loss: 0.3408 - val_accuracy: 0.9000 - val_loss: 0.2882\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8423 - loss: 0.3469 - val_accuracy: 0.9000 - val_loss: 0.2804\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9042 - loss: 0.3092 - val_accuracy: 0.9000 - val_loss: 0.2726\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8656 - loss: 0.3259 - val_accuracy: 0.9000 - val_loss: 0.2652\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8869 - loss: 0.3230 - val_accuracy: 0.9333 - val_loss: 0.2581\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8923 - loss: 0.3158 - val_accuracy: 0.9333 - val_loss: 0.2510\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8810 - loss: 0.3147 - val_accuracy: 0.9333 - val_loss: 0.2446\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9156 - loss: 0.2735 - val_accuracy: 0.9333 - val_loss: 0.2383\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9073 - loss: 0.3091 - val_accuracy: 0.9333 - val_loss: 0.2316\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8990 - loss: 0.2994 - val_accuracy: 0.9333 - val_loss: 0.2256\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9190 - loss: 0.2660 - val_accuracy: 0.9333 - val_loss: 0.2193\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9056 - loss: 0.2816 - val_accuracy: 0.9667 - val_loss: 0.2130\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9410 - loss: 0.2514 - val_accuracy: 0.9667 - val_loss: 0.2063\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9308 - loss: 0.2583 - val_accuracy: 0.9667 - val_loss: 0.2004\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9329 - loss: 0.2552 - val_accuracy: 1.0000 - val_loss: 0.1943\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9558 - loss: 0.2079 - val_accuracy: 1.0000 - val_loss: 0.1888\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9529 - loss: 0.2321 - val_accuracy: 1.0000 - val_loss: 0.1834\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9560 - loss: 0.2172 - val_accuracy: 1.0000 - val_loss: 0.1781\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9383 - loss: 0.2255 - val_accuracy: 1.0000 - val_loss: 0.1728\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9435 - loss: 0.2087 - val_accuracy: 1.0000 - val_loss: 0.1671\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9654 - loss: 0.1909 - val_accuracy: 1.0000 - val_loss: 0.1619\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9581 - loss: 0.1903 - val_accuracy: 1.0000 - val_loss: 0.1571\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9610 - loss: 0.1803 - val_accuracy: 1.0000 - val_loss: 0.1522\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9571 - loss: 0.1825 - val_accuracy: 1.0000 - val_loss: 0.1479\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9385 - loss: 0.2022 - val_accuracy: 1.0000 - val_loss: 0.1445\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9625 - loss: 0.1770 - val_accuracy: 1.0000 - val_loss: 0.1403\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9573 - loss: 0.1834 - val_accuracy: 1.0000 - val_loss: 0.1374\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9479 - loss: 0.1741 - val_accuracy: 1.0000 - val_loss: 0.1338\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9437 - loss: 0.1735 - val_accuracy: 1.0000 - val_loss: 0.1299\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9656 - loss: 0.1437 - val_accuracy: 1.0000 - val_loss: 0.1263\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9435 - loss: 0.1657 - val_accuracy: 1.0000 - val_loss: 0.1232\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9488 - loss: 0.1541 - val_accuracy: 1.0000 - val_loss: 0.1204\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9560 - loss: 0.1390 - val_accuracy: 1.0000 - val_loss: 0.1171\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9615 - loss: 0.1427 - val_accuracy: 1.0000 - val_loss: 0.1156\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.9531 - loss: 0.1486 - val_accuracy: 1.0000 - val_loss: 0.1124\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9654 - loss: 0.1303 - val_accuracy: 1.0000 - val_loss: 0.1101\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9488 - loss: 0.1391 - val_accuracy: 1.0000 - val_loss: 0.1075\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9498 - loss: 0.1366 - val_accuracy: 1.0000 - val_loss: 0.1051\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.9383 - loss: 0.1309 - val_accuracy: 1.0000 - val_loss: 0.1029\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9571 - loss: 0.1188 - val_accuracy: 1.0000 - val_loss: 0.1011\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9479 - loss: 0.1420 - val_accuracy: 1.0000 - val_loss: 0.0991\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9477 - loss: 0.1380 - val_accuracy: 1.0000 - val_loss: 0.0969\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9540 - loss: 0.1091 - val_accuracy: 1.0000 - val_loss: 0.0956\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9571 - loss: 0.1122 - val_accuracy: 1.0000 - val_loss: 0.0937\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9542 - loss: 0.1141 - val_accuracy: 1.0000 - val_loss: 0.0928\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9625 - loss: 0.1013 - val_accuracy: 1.0000 - val_loss: 0.0910\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9794 - loss: 0.1030 - val_accuracy: 1.0000 - val_loss: 0.0899\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9658 - loss: 0.1141 - val_accuracy: 1.0000 - val_loss: 0.0884\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9617 - loss: 0.1276 - val_accuracy: 1.0000 - val_loss: 0.0866\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9437 - loss: 0.1285 - val_accuracy: 1.0000 - val_loss: 0.0849\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9573 - loss: 0.1083 - val_accuracy: 1.0000 - val_loss: 0.0833\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9594 - loss: 0.1013 - val_accuracy: 1.0000 - val_loss: 0.0808\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9573 - loss: 0.1028 - val_accuracy: 1.0000 - val_loss: 0.0798\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9563 - loss: 0.0954 - val_accuracy: 1.0000 - val_loss: 0.0794\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9596 - loss: 0.0956 - val_accuracy: 1.0000 - val_loss: 0.0785\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9617 - loss: 0.1041 - val_accuracy: 1.0000 - val_loss: 0.0770\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.9596 - loss: 0.0959 - val_accuracy: 1.0000 - val_loss: 0.0762\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9552 - loss: 0.0986 - val_accuracy: 1.0000 - val_loss: 0.0756\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9531 - loss: 0.0980 - val_accuracy: 1.0000 - val_loss: 0.0748\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9785 - loss: 0.0898 - val_accuracy: 1.0000 - val_loss: 0.0745\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.9848 - loss: 0.0843 - val_accuracy: 1.0000 - val_loss: 0.0730\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9690 - loss: 0.0898 - val_accuracy: 1.0000 - val_loss: 0.0727\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 1.0000 - loss: 0.0727\n",
      "Test Loss: 0.07272713631391525, Test Accuracy: 1.0\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
      "Predicted labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.activations import relu, sigmoid, tanh, softmax, elu, swish\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # features\n",
    "y = data.target  # labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Encode the labels\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer with ReLU activation\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "\n",
    "# Second hidden layer with Leaky ReLU activation\n",
    "model.add(Dense(10, activation=LeakyReLU(alpha=0.1)))\n",
    "\n",
    "# Third hidden layer with Tanh activation\n",
    "model.add(Dense(10, activation='tanh'))\n",
    "\n",
    "# Output layer with Softmax activation (for multi-class classification)\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',  # For multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict class labels for the test data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b502bcae-e4fb-4ad4-8612-ed5a1980cdaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 173ms/step - accuracy: 0.2902 - loss: 1.1705 - val_accuracy: 0.4333 - val_loss: 1.1263\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3475 - loss: 1.1443 - val_accuracy: 0.4333 - val_loss: 1.1060\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.3215 - loss: 1.1090 - val_accuracy: 0.5000 - val_loss: 1.0870\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3408 - loss: 1.1143 - val_accuracy: 0.5000 - val_loss: 1.0693\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.3652 - loss: 1.1184 - val_accuracy: 0.5333 - val_loss: 1.0522\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4594 - loss: 1.0795 - val_accuracy: 0.6000 - val_loss: 1.0352\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4294 - loss: 1.0496 - val_accuracy: 0.6333 - val_loss: 1.0189\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5225 - loss: 1.0190 - val_accuracy: 0.6333 - val_loss: 1.0024\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4117 - loss: 1.0641 - val_accuracy: 0.6333 - val_loss: 0.9863\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4210 - loss: 1.0579 - val_accuracy: 0.6667 - val_loss: 0.9706\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4737 - loss: 1.0384 - val_accuracy: 0.6667 - val_loss: 0.9548\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5215 - loss: 0.9922 - val_accuracy: 0.6667 - val_loss: 0.9388\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5150 - loss: 0.9953 - val_accuracy: 0.7333 - val_loss: 0.9233\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4654 - loss: 1.0049 - val_accuracy: 0.7333 - val_loss: 0.9081\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.5323 - loss: 0.9568 - val_accuracy: 0.7333 - val_loss: 0.8930\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5646 - loss: 0.9515 - val_accuracy: 0.7333 - val_loss: 0.8781\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6790 - loss: 0.8728 - val_accuracy: 0.7667 - val_loss: 0.8636\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6006 - loss: 0.9148 - val_accuracy: 0.8000 - val_loss: 0.8494\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6146 - loss: 0.8989 - val_accuracy: 0.8000 - val_loss: 0.8355\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6044 - loss: 0.8705 - val_accuracy: 0.8000 - val_loss: 0.8213\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6190 - loss: 0.8596 - val_accuracy: 0.8000 - val_loss: 0.8072\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7071 - loss: 0.8531 - val_accuracy: 0.8333 - val_loss: 0.7929\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6631 - loss: 0.8609 - val_accuracy: 0.8333 - val_loss: 0.7786\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6419 - loss: 0.8521 - val_accuracy: 0.8333 - val_loss: 0.7642\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6617 - loss: 0.8233 - val_accuracy: 0.8333 - val_loss: 0.7500\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5515 - loss: 0.9026 - val_accuracy: 0.8667 - val_loss: 0.7372\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6046 - loss: 0.8248 - val_accuracy: 0.9000 - val_loss: 0.7246\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.6367 - loss: 0.8103 - val_accuracy: 0.9000 - val_loss: 0.7116\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.6994 - loss: 0.7571 - val_accuracy: 0.9000 - val_loss: 0.6982\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6585 - loss: 0.7721 - val_accuracy: 0.9000 - val_loss: 0.6850\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7223 - loss: 0.7363 - val_accuracy: 0.9000 - val_loss: 0.6716\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.6258 - loss: 0.7918 - val_accuracy: 0.9000 - val_loss: 0.6584\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7069 - loss: 0.7564 - val_accuracy: 0.9000 - val_loss: 0.6446\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7135 - loss: 0.7277 - val_accuracy: 0.9000 - val_loss: 0.6311\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7163 - loss: 0.6835 - val_accuracy: 0.9000 - val_loss: 0.6184\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7135 - loss: 0.7120 - val_accuracy: 0.9000 - val_loss: 0.6057\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7158 - loss: 0.7208 - val_accuracy: 0.9000 - val_loss: 0.5932\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.7285 - loss: 0.6980 - val_accuracy: 0.9000 - val_loss: 0.5807\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7306 - loss: 0.6701 - val_accuracy: 0.9000 - val_loss: 0.5688\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.7383 - loss: 0.6888 - val_accuracy: 0.9000 - val_loss: 0.5570\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7817 - loss: 0.5968 - val_accuracy: 0.9000 - val_loss: 0.5450\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7792 - loss: 0.6466 - val_accuracy: 0.9333 - val_loss: 0.5339\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7229 - loss: 0.7028 - val_accuracy: 0.9000 - val_loss: 0.5232\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.6708 - loss: 0.7108 - val_accuracy: 0.9333 - val_loss: 0.5128\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6969 - loss: 0.6389 - val_accuracy: 0.9333 - val_loss: 0.5029\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7283 - loss: 0.6536 - val_accuracy: 0.9000 - val_loss: 0.4932\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8037 - loss: 0.6238 - val_accuracy: 0.9000 - val_loss: 0.4835\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7185 - loss: 0.6641 - val_accuracy: 0.9000 - val_loss: 0.4742\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7735 - loss: 0.6408 - val_accuracy: 0.9000 - val_loss: 0.4651\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7183 - loss: 0.6190 - val_accuracy: 0.9000 - val_loss: 0.4563\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7435 - loss: 0.6113 - val_accuracy: 0.9333 - val_loss: 0.4483\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7629 - loss: 0.5768 - val_accuracy: 0.9333 - val_loss: 0.4403\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7892 - loss: 0.5039 - val_accuracy: 0.9333 - val_loss: 0.4323\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7875 - loss: 0.5383 - val_accuracy: 0.9000 - val_loss: 0.4244\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7344 - loss: 0.6293 - val_accuracy: 0.9000 - val_loss: 0.4167\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7202 - loss: 0.6341 - val_accuracy: 0.9000 - val_loss: 0.4095\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8471 - loss: 0.5308 - val_accuracy: 0.9333 - val_loss: 0.4018\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7502 - loss: 0.5508 - val_accuracy: 0.9333 - val_loss: 0.3945\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8010 - loss: 0.5118 - val_accuracy: 0.9333 - val_loss: 0.3874\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7663 - loss: 0.5298 - val_accuracy: 0.9333 - val_loss: 0.3809\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7571 - loss: 0.5676 - val_accuracy: 0.9333 - val_loss: 0.3744\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7944 - loss: 0.5194 - val_accuracy: 0.9333 - val_loss: 0.3680\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8444 - loss: 0.5179 - val_accuracy: 0.9333 - val_loss: 0.3615\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8392 - loss: 0.4670 - val_accuracy: 0.9333 - val_loss: 0.3549\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7631 - loss: 0.5140 - val_accuracy: 0.9333 - val_loss: 0.3492\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8194 - loss: 0.5458 - val_accuracy: 0.9333 - val_loss: 0.3437\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.7144 - loss: 0.5832 - val_accuracy: 0.9333 - val_loss: 0.3388\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7927 - loss: 0.4735 - val_accuracy: 0.9333 - val_loss: 0.3341\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7506 - loss: 0.5484 - val_accuracy: 0.9333 - val_loss: 0.3302\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.7565 - loss: 0.5601 - val_accuracy: 0.9333 - val_loss: 0.3265\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7979 - loss: 0.5241 - val_accuracy: 0.9333 - val_loss: 0.3225\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7923 - loss: 0.4473 - val_accuracy: 0.9333 - val_loss: 0.3186\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8465 - loss: 0.4308 - val_accuracy: 0.9333 - val_loss: 0.3148\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7590 - loss: 0.5120 - val_accuracy: 0.9333 - val_loss: 0.3112\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8758 - loss: 0.4280 - val_accuracy: 0.9000 - val_loss: 0.3074\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8110 - loss: 0.4595 - val_accuracy: 0.9000 - val_loss: 0.3037\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8325 - loss: 0.4478 - val_accuracy: 0.9000 - val_loss: 0.2998\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7919 - loss: 0.4720 - val_accuracy: 0.9000 - val_loss: 0.2961\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7679 - loss: 0.4787 - val_accuracy: 0.9333 - val_loss: 0.2922\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7998 - loss: 0.5262 - val_accuracy: 0.9333 - val_loss: 0.2888\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7746 - loss: 0.4718 - val_accuracy: 0.9333 - val_loss: 0.2854\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8256 - loss: 0.4110 - val_accuracy: 0.9333 - val_loss: 0.2821\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8054 - loss: 0.4504 - val_accuracy: 0.9333 - val_loss: 0.2790\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7867 - loss: 0.5079 - val_accuracy: 0.9333 - val_loss: 0.2756\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7710 - loss: 0.4312 - val_accuracy: 0.9333 - val_loss: 0.2719\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7860 - loss: 0.4592 - val_accuracy: 0.9333 - val_loss: 0.2685\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7923 - loss: 0.4644 - val_accuracy: 0.9667 - val_loss: 0.2649\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8352 - loss: 0.4483 - val_accuracy: 0.9667 - val_loss: 0.2614\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8327 - loss: 0.4125 - val_accuracy: 0.9667 - val_loss: 0.2581\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8023 - loss: 0.4557 - val_accuracy: 0.9333 - val_loss: 0.2554\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8077 - loss: 0.4717 - val_accuracy: 0.9333 - val_loss: 0.2531\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7688 - loss: 0.4919 - val_accuracy: 0.9333 - val_loss: 0.2508\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7823 - loss: 0.5062 - val_accuracy: 0.9333 - val_loss: 0.2486\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8371 - loss: 0.3825 - val_accuracy: 0.9667 - val_loss: 0.2466\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8498 - loss: 0.4125 - val_accuracy: 0.9667 - val_loss: 0.2448\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8692 - loss: 0.4071 - val_accuracy: 0.9667 - val_loss: 0.2429\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.9102 - loss: 0.3421 - val_accuracy: 0.9667 - val_loss: 0.2408\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8035 - loss: 0.4617 - val_accuracy: 0.9667 - val_loss: 0.2391\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8531 - loss: 0.3817 - val_accuracy: 0.9667 - val_loss: 0.2373\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8683 - loss: 0.3861 - val_accuracy: 0.9667 - val_loss: 0.2352\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.9667 - loss: 0.2352\n",
      "Test Loss: 0.23523221909999847, Test Accuracy: 0.9666666388511658\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "Predicted labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # features\n",
    "y = data.target  # labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Encode the labels (One-hot encoding for multi-class)\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer with ReLU activation\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))\n",
    "\n",
    "# Apply Dropout (20% of neurons will be dropped)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second hidden layer with ReLU activation\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "# Apply Dropout (20% of neurons will be dropped)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer with Softmax activation for multi-class classification\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',  # For multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict class labels for the test data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "879f7fb8-1eb6-45e1-a5ec-4d752636ebc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer activations (a1):\n",
      "[[0.         0.10211419 0.24158611 0.16607101]\n",
      " [0.         0.15316183 0.77063121 0.71349145]\n",
      " [0.         0.20420946 1.29967631 1.2609119 ]\n",
      " [0.         0.06804459 1.52189755 1.65866383]]\n",
      "\n",
      "Final output layer activations (a2):\n",
      "[[0.33060082]\n",
      " [0.11675712]\n",
      " [0.0341733 ]\n",
      " [0.02441159]]\n"
     ]
    }
   ],
   "source": [
    "#6.\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Define the activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Step 2: Define the forward propagation function\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Layer 1: Input to Hidden Layer\n",
    "    z1 = np.dot(X, W1) + b1   # Linear transformation\n",
    "    a1 = relu(z1)             # Apply ReLU activation function\n",
    "\n",
    "    # Layer 2: Hidden Layer to Output Layer\n",
    "    z2 = np.dot(a1, W2) + b2   # Linear transformation\n",
    "    a2 = sigmoid(z2)           # Apply Sigmoid activation function (for binary classification)\n",
    "\n",
    "    return a1, a2  # Return hidden layer activations and final output\n",
    "\n",
    "# Step 3: Initialize the network parameters (weights and biases)\n",
    "input_size = 3   # Number of input features\n",
    "hidden_size = 4  # Number of neurons in hidden layer\n",
    "output_size = 1  # Output size (for binary classification)\n",
    "\n",
    "# Randomly initialize weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Weights from input to hidden layer\n",
    "b1 = np.zeros((1, hidden_size))                 # Bias for hidden layer\n",
    "W2 = np.random.randn(hidden_size, output_size)  # Weights from hidden to output layer\n",
    "b2 = np.zeros((1, output_size))                 # Bias for output layer\n",
    "\n",
    "# Step 4: Example input data (let's assume 4 examples with 3 features each)\n",
    "X = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.4, 0.5, 0.6],\n",
    "              [0.7, 0.8, 0.9],\n",
    "              [0.9, 0.8, 0.7]])\n",
    "\n",
    "# Step 5: Perform forward propagation\n",
    "a1, a2 = forward_propagation(X, W1, b1, W2, b2)\n",
    "\n",
    "# Output activations for the hidden layer and the final output layer\n",
    "print(\"Hidden layer activations (a1):\")\n",
    "print(a1)\n",
    "print(\"\\nFinal output layer activations (a2):\")\n",
    "print(a2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaed0a92-0553-4885-a201-a5e21f7e8caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 189ms/step - accuracy: 0.3442 - loss: 1.1703 - val_accuracy: 0.3667 - val_loss: 1.0542\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4106 - loss: 1.0952 - val_accuracy: 0.4333 - val_loss: 1.0204\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.3952 - loss: 1.0540 - val_accuracy: 0.4000 - val_loss: 0.9905\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4152 - loss: 1.0006 - val_accuracy: 0.4000 - val_loss: 0.9625\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4060 - loss: 0.9792 - val_accuracy: 0.4000 - val_loss: 0.9356\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4137 - loss: 0.9642 - val_accuracy: 0.4000 - val_loss: 0.9104\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4394 - loss: 0.9024 - val_accuracy: 0.4000 - val_loss: 0.8864\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4469 - loss: 0.8704 - val_accuracy: 0.5333 - val_loss: 0.8630\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4860 - loss: 0.8743 - val_accuracy: 0.5667 - val_loss: 0.8412\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4938 - loss: 0.8270 - val_accuracy: 0.6333 - val_loss: 0.8213\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.6258 - loss: 0.7825 - val_accuracy: 0.6667 - val_loss: 0.8025\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.5777 - loss: 0.8056 - val_accuracy: 0.7000 - val_loss: 0.7851\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6300 - loss: 0.7812 - val_accuracy: 0.7000 - val_loss: 0.7682\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6817 - loss: 0.7228 - val_accuracy: 0.7000 - val_loss: 0.7520\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.7142 - loss: 0.7379 - val_accuracy: 0.7000 - val_loss: 0.7365\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7644 - loss: 0.6745 - val_accuracy: 0.7000 - val_loss: 0.7212\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7456 - loss: 0.6972 - val_accuracy: 0.7000 - val_loss: 0.7063\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.7429 - loss: 0.6634 - val_accuracy: 0.7000 - val_loss: 0.6910\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7806 - loss: 0.6360 - val_accuracy: 0.7333 - val_loss: 0.6755\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7765 - loss: 0.6412 - val_accuracy: 0.7333 - val_loss: 0.6605\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8288 - loss: 0.6206 - val_accuracy: 0.7333 - val_loss: 0.6455\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7525 - loss: 0.6626 - val_accuracy: 0.7333 - val_loss: 0.6315\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.7613 - loss: 0.5993 - val_accuracy: 0.7333 - val_loss: 0.6176\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8394 - loss: 0.5611 - val_accuracy: 0.7333 - val_loss: 0.6044\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.7983 - loss: 0.6038 - val_accuracy: 0.7667 - val_loss: 0.5916\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7738 - loss: 0.5603 - val_accuracy: 0.8000 - val_loss: 0.5787\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.7819 - loss: 0.6127 - val_accuracy: 0.8000 - val_loss: 0.5662\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.7919 - loss: 0.5363 - val_accuracy: 0.8000 - val_loss: 0.5540\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8173 - loss: 0.5523 - val_accuracy: 0.8000 - val_loss: 0.5420\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8256 - loss: 0.5610 - val_accuracy: 0.8333 - val_loss: 0.5304\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.8279 - loss: 0.5063 - val_accuracy: 0.8333 - val_loss: 0.5196\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8669 - loss: 0.4861 - val_accuracy: 0.8333 - val_loss: 0.5092\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.7721 - loss: 0.5886 - val_accuracy: 0.8333 - val_loss: 0.4991\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8215 - loss: 0.4941 - val_accuracy: 0.8333 - val_loss: 0.4887\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8415 - loss: 0.4894 - val_accuracy: 0.8333 - val_loss: 0.4790\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8496 - loss: 0.4885 - val_accuracy: 0.8333 - val_loss: 0.4694\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8169 - loss: 0.5360 - val_accuracy: 0.8333 - val_loss: 0.4611\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8585 - loss: 0.4413 - val_accuracy: 0.8333 - val_loss: 0.4525\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8310 - loss: 0.4919 - val_accuracy: 0.8333 - val_loss: 0.4451\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8283 - loss: 0.4641 - val_accuracy: 0.8333 - val_loss: 0.4379\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8621 - loss: 0.4406 - val_accuracy: 0.8333 - val_loss: 0.4313\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8879 - loss: 0.4131 - val_accuracy: 0.8333 - val_loss: 0.4247\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8490 - loss: 0.4319 - val_accuracy: 0.8333 - val_loss: 0.4184\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8394 - loss: 0.4611 - val_accuracy: 0.8333 - val_loss: 0.4127\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.8410 - loss: 0.4431 - val_accuracy: 0.8333 - val_loss: 0.4060\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8771 - loss: 0.3932 - val_accuracy: 0.8333 - val_loss: 0.3996\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8548 - loss: 0.3945 - val_accuracy: 0.8333 - val_loss: 0.3937\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8635 - loss: 0.3886 - val_accuracy: 0.8333 - val_loss: 0.3877\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9077 - loss: 0.3806 - val_accuracy: 0.8667 - val_loss: 0.3820\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9087 - loss: 0.3814 - val_accuracy: 0.8667 - val_loss: 0.3772\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8400 - loss: 0.4086 - val_accuracy: 0.8667 - val_loss: 0.3720\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9060 - loss: 0.3773 - val_accuracy: 0.8667 - val_loss: 0.3654\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8858 - loss: 0.4000 - val_accuracy: 0.8667 - val_loss: 0.3598\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8921 - loss: 0.3805 - val_accuracy: 0.8667 - val_loss: 0.3531\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8892 - loss: 0.3378 - val_accuracy: 0.8667 - val_loss: 0.3470\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9015 - loss: 0.3952 - val_accuracy: 0.8667 - val_loss: 0.3406\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9254 - loss: 0.3404 - val_accuracy: 0.8667 - val_loss: 0.3335\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8879 - loss: 0.3387 - val_accuracy: 0.8667 - val_loss: 0.3273\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8646 - loss: 0.3804 - val_accuracy: 0.8667 - val_loss: 0.3220\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.9483 - loss: 0.2799 - val_accuracy: 0.8667 - val_loss: 0.3166\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9123 - loss: 0.3285 - val_accuracy: 0.8667 - val_loss: 0.3118\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9167 - loss: 0.2927 - val_accuracy: 0.8667 - val_loss: 0.3070\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9333 - loss: 0.2952 - val_accuracy: 0.8667 - val_loss: 0.3016\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9219 - loss: 0.2977 - val_accuracy: 0.8667 - val_loss: 0.2968\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8804 - loss: 0.3494 - val_accuracy: 0.8667 - val_loss: 0.2921\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9548 - loss: 0.2455 - val_accuracy: 0.8667 - val_loss: 0.2859\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9352 - loss: 0.2748 - val_accuracy: 0.8667 - val_loss: 0.2799\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9273 - loss: 0.2666 - val_accuracy: 0.9000 - val_loss: 0.2739\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9235 - loss: 0.2664 - val_accuracy: 0.9000 - val_loss: 0.2681\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9006 - loss: 0.2884 - val_accuracy: 0.9000 - val_loss: 0.2626\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9056 - loss: 0.2814 - val_accuracy: 0.9000 - val_loss: 0.2580\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8977 - loss: 0.2837 - val_accuracy: 0.9000 - val_loss: 0.2538\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9646 - loss: 0.2671 - val_accuracy: 0.9333 - val_loss: 0.2498\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9000 - loss: 0.3176 - val_accuracy: 0.9333 - val_loss: 0.2463\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9773 - loss: 0.1920 - val_accuracy: 0.9333 - val_loss: 0.2405\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9583 - loss: 0.2339 - val_accuracy: 0.9333 - val_loss: 0.2354\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9773 - loss: 0.1844 - val_accuracy: 0.9333 - val_loss: 0.2298\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9806 - loss: 0.1957 - val_accuracy: 0.9333 - val_loss: 0.2240\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9129 - loss: 0.2564 - val_accuracy: 0.9333 - val_loss: 0.2180\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.9635 - loss: 0.1918 - val_accuracy: 0.9333 - val_loss: 0.2108\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9563 - loss: 0.2104 - val_accuracy: 0.9667 - val_loss: 0.2039\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.9712 - loss: 0.1838 - val_accuracy: 0.9667 - val_loss: 0.1979\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9798 - loss: 0.1742 - val_accuracy: 0.9667 - val_loss: 0.1921\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9675 - loss: 0.2398 - val_accuracy: 0.9667 - val_loss: 0.1870\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9594 - loss: 0.1991 - val_accuracy: 0.9667 - val_loss: 0.1805\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9423 - loss: 0.2000 - val_accuracy: 0.9667 - val_loss: 0.1742\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9465 - loss: 0.2154 - val_accuracy: 0.9667 - val_loss: 0.1693\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9642 - loss: 0.1631 - val_accuracy: 0.9667 - val_loss: 0.1648\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9446 - loss: 0.1795 - val_accuracy: 0.9667 - val_loss: 0.1612\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9592 - loss: 0.1479 - val_accuracy: 0.9667 - val_loss: 0.1573\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9675 - loss: 0.1619 - val_accuracy: 0.9667 - val_loss: 0.1548\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9744 - loss: 0.1709 - val_accuracy: 0.9667 - val_loss: 0.1514\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9827 - loss: 0.1575 - val_accuracy: 0.9667 - val_loss: 0.1482\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8996 - loss: 0.2363 - val_accuracy: 0.9667 - val_loss: 0.1452\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9792 - loss: 0.1362 - val_accuracy: 0.9667 - val_loss: 0.1424\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9302 - loss: 0.2027 - val_accuracy: 0.9667 - val_loss: 0.1402\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.9633 - loss: 0.1533 - val_accuracy: 0.9667 - val_loss: 0.1392\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9385 - loss: 0.1928 - val_accuracy: 0.9667 - val_loss: 0.1372\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9698 - loss: 0.1200 - val_accuracy: 0.9667 - val_loss: 0.1334\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9698 - loss: 0.1368 - val_accuracy: 0.9667 - val_loss: 0.1314\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9667 - loss: 0.1314\n",
      "Test Loss: 0.13137522339820862, Test Accuracy: 0.9666666388511658\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220ms/step\n",
      "Predicted labels: [1 0 2 1 1 0 1 2 2 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "#7.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # features\n",
    "y = data.target  # labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Encode the labels (One-hot encoding for multi-class)\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer with Batch Normalization\n",
    "model.add(Dense(10, input_dim=4))  # Dense layer with 10 neurons\n",
    "model.add(BatchNormalization())    # Apply Batch Normalization\n",
    "model.add(tf.keras.layers.ReLU())  # Apply ReLU activation\n",
    "\n",
    "# Second hidden layer with Batch Normalization\n",
    "model.add(Dense(10))               # Dense layer with 10 neurons\n",
    "model.add(BatchNormalization())    # Apply Batch Normalization\n",
    "model.add(tf.keras.layers.ReLU())  # Apply ReLU activation\n",
    "\n",
    "# Output layer with Softmax activation for multi-class classification\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',  # For multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict class labels for the test data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "778d8096-99cb-437a-afdf-e1091badfeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 159ms/step - accuracy: 0.2015 - loss: 1.0728 - val_accuracy: 0.2000 - val_loss: 1.0422\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.3352 - loss: 1.0427 - val_accuracy: 0.3333 - val_loss: 1.0273\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3554 - loss: 1.0315 - val_accuracy: 0.4000 - val_loss: 1.0125\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4425 - loss: 1.0187 - val_accuracy: 0.5667 - val_loss: 0.9975\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4812 - loss: 1.0171 - val_accuracy: 0.5667 - val_loss: 0.9827\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5281 - loss: 0.9978 - val_accuracy: 0.5667 - val_loss: 0.9681\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5831 - loss: 0.9796 - val_accuracy: 0.6000 - val_loss: 0.9536\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5790 - loss: 0.9821 - val_accuracy: 0.6333 - val_loss: 0.9392\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6221 - loss: 0.9558 - val_accuracy: 0.7333 - val_loss: 0.9245\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.6652 - loss: 0.9329 - val_accuracy: 0.8333 - val_loss: 0.9094\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6552 - loss: 0.9305 - val_accuracy: 0.8667 - val_loss: 0.8944\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6673 - loss: 0.9184 - val_accuracy: 0.8667 - val_loss: 0.8791\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.7223 - loss: 0.9047 - val_accuracy: 0.8667 - val_loss: 0.8635\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7446 - loss: 0.8735 - val_accuracy: 0.8667 - val_loss: 0.8480\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7531 - loss: 0.8499 - val_accuracy: 0.8667 - val_loss: 0.8321\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7304 - loss: 0.8745 - val_accuracy: 0.8667 - val_loss: 0.8160\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7448 - loss: 0.8324 - val_accuracy: 0.8667 - val_loss: 0.7995\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.7638 - loss: 0.8330 - val_accuracy: 0.8667 - val_loss: 0.7832\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.7710 - loss: 0.8043 - val_accuracy: 0.8667 - val_loss: 0.7667\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.7554 - loss: 0.8008 - val_accuracy: 0.8667 - val_loss: 0.7501\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.7377 - loss: 0.7978 - val_accuracy: 0.8667 - val_loss: 0.7337\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7462 - loss: 0.7775 - val_accuracy: 0.8667 - val_loss: 0.7173\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8225 - loss: 0.7305 - val_accuracy: 0.8667 - val_loss: 0.7005\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8113 - loss: 0.7334 - val_accuracy: 0.8667 - val_loss: 0.6841\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8167 - loss: 0.7169 - val_accuracy: 0.8667 - val_loss: 0.6678\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.7844 - loss: 0.7091 - val_accuracy: 0.8667 - val_loss: 0.6512\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8054 - loss: 0.6657 - val_accuracy: 0.8667 - val_loss: 0.6348\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8117 - loss: 0.6480 - val_accuracy: 0.8667 - val_loss: 0.6180\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8492 - loss: 0.6204 - val_accuracy: 0.8667 - val_loss: 0.6006\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7931 - loss: 0.6512 - val_accuracy: 0.8667 - val_loss: 0.5840\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8306 - loss: 0.6068 - val_accuracy: 0.8667 - val_loss: 0.5674\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8054 - loss: 0.5774 - val_accuracy: 0.8667 - val_loss: 0.5511\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8010 - loss: 0.5725 - val_accuracy: 0.8667 - val_loss: 0.5351\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8000 - loss: 0.5513 - val_accuracy: 0.8667 - val_loss: 0.5196\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7667 - loss: 0.5536 - val_accuracy: 0.8667 - val_loss: 0.5044\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8167 - loss: 0.5366 - val_accuracy: 0.8667 - val_loss: 0.4895\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.7885 - loss: 0.5229 - val_accuracy: 0.8667 - val_loss: 0.4752\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8338 - loss: 0.4928 - val_accuracy: 0.8667 - val_loss: 0.4605\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7973 - loss: 0.5051 - val_accuracy: 0.8667 - val_loss: 0.4465\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8121 - loss: 0.5083 - val_accuracy: 0.8667 - val_loss: 0.4335\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8267 - loss: 0.4710 - val_accuracy: 0.8667 - val_loss: 0.4214\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8321 - loss: 0.4589 - val_accuracy: 0.8667 - val_loss: 0.4103\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8490 - loss: 0.4451 - val_accuracy: 0.8667 - val_loss: 0.3996\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8438 - loss: 0.4464 - val_accuracy: 0.8667 - val_loss: 0.3893\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8200 - loss: 0.4488 - val_accuracy: 0.8667 - val_loss: 0.3799\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8377 - loss: 0.4493 - val_accuracy: 0.8667 - val_loss: 0.3709\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8283 - loss: 0.4334 - val_accuracy: 0.8667 - val_loss: 0.3628\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8502 - loss: 0.4086 - val_accuracy: 0.8667 - val_loss: 0.3551\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8408 - loss: 0.4138 - val_accuracy: 0.8667 - val_loss: 0.3478\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8325 - loss: 0.3963 - val_accuracy: 0.8667 - val_loss: 0.3412\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8263 - loss: 0.3967 - val_accuracy: 0.8667 - val_loss: 0.3351\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8877 - loss: 0.3417 - val_accuracy: 0.8667 - val_loss: 0.3289\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8408 - loss: 0.3690 - val_accuracy: 0.8667 - val_loss: 0.3232\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8315 - loss: 0.3868 - val_accuracy: 0.8667 - val_loss: 0.3177\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8533 - loss: 0.3458 - val_accuracy: 0.8667 - val_loss: 0.3125\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8273 - loss: 0.3730 - val_accuracy: 0.8667 - val_loss: 0.3077\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8419 - loss: 0.3523 - val_accuracy: 0.8667 - val_loss: 0.3030\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8398 - loss: 0.3516 - val_accuracy: 0.8667 - val_loss: 0.2986\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8294 - loss: 0.3680 - val_accuracy: 0.9000 - val_loss: 0.2943\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8377 - loss: 0.3295 - val_accuracy: 0.9000 - val_loss: 0.2901\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8419 - loss: 0.3611 - val_accuracy: 0.9000 - val_loss: 0.2860\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8398 - loss: 0.3165 - val_accuracy: 0.9000 - val_loss: 0.2822\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8335 - loss: 0.3508 - val_accuracy: 0.9000 - val_loss: 0.2787\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8325 - loss: 0.3248 - val_accuracy: 0.9000 - val_loss: 0.2752\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8796 - loss: 0.2991 - val_accuracy: 0.9000 - val_loss: 0.2719\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8702 - loss: 0.3097 - val_accuracy: 0.9000 - val_loss: 0.2685\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8473 - loss: 0.3177 - val_accuracy: 0.9000 - val_loss: 0.2654\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8390 - loss: 0.3118 - val_accuracy: 0.9000 - val_loss: 0.2621\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8619 - loss: 0.2799 - val_accuracy: 0.9000 - val_loss: 0.2589\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8254 - loss: 0.3111 - val_accuracy: 0.9000 - val_loss: 0.2560\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8379 - loss: 0.3186 - val_accuracy: 0.9000 - val_loss: 0.2531\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8546 - loss: 0.2965 - val_accuracy: 0.9000 - val_loss: 0.2501\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8567 - loss: 0.3120 - val_accuracy: 0.9000 - val_loss: 0.2472\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8652 - loss: 0.2837 - val_accuracy: 0.9000 - val_loss: 0.2444\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.8746 - loss: 0.2763 - val_accuracy: 0.9000 - val_loss: 0.2416\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8756 - loss: 0.2648 - val_accuracy: 0.9000 - val_loss: 0.2390\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8652 - loss: 0.2941 - val_accuracy: 0.9000 - val_loss: 0.2365\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8569 - loss: 0.2883 - val_accuracy: 0.9000 - val_loss: 0.2338\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8788 - loss: 0.2542 - val_accuracy: 0.9000 - val_loss: 0.2311\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8673 - loss: 0.2766 - val_accuracy: 0.9000 - val_loss: 0.2284\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8769 - loss: 0.2546 - val_accuracy: 0.9000 - val_loss: 0.2260\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8602 - loss: 0.2750 - val_accuracy: 0.9000 - val_loss: 0.2233\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8844 - loss: 0.2604 - val_accuracy: 0.9000 - val_loss: 0.2208\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8719 - loss: 0.2720 - val_accuracy: 0.9000 - val_loss: 0.2183\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.8792 - loss: 0.2701 - val_accuracy: 0.9000 - val_loss: 0.2159\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8896 - loss: 0.2495 - val_accuracy: 0.9000 - val_loss: 0.2131\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8900 - loss: 0.2604 - val_accuracy: 0.9000 - val_loss: 0.2106\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8994 - loss: 0.2399 - val_accuracy: 0.9000 - val_loss: 0.2081\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8817 - loss: 0.2617 - val_accuracy: 0.9000 - val_loss: 0.2058\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8850 - loss: 0.2652 - val_accuracy: 0.9333 - val_loss: 0.2031\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9008 - loss: 0.2615 - val_accuracy: 0.9333 - val_loss: 0.2005\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.9133 - loss: 0.2372 - val_accuracy: 0.9333 - val_loss: 0.1983\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8988 - loss: 0.2384 - val_accuracy: 0.9333 - val_loss: 0.1956\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.9167 - loss: 0.2629 - val_accuracy: 0.9333 - val_loss: 0.1930\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9135 - loss: 0.2606 - val_accuracy: 0.9333 - val_loss: 0.1906\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9167 - loss: 0.2455 - val_accuracy: 0.9333 - val_loss: 0.1882\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.8990 - loss: 0.2562 - val_accuracy: 0.9333 - val_loss: 0.1859\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.9198 - loss: 0.2352 - val_accuracy: 0.9333 - val_loss: 0.1835\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9469 - loss: 0.2164 - val_accuracy: 0.9333 - val_loss: 0.1810\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.9062 - loss: 0.2565 - val_accuracy: 0.9333 - val_loss: 0.1785\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#8\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # features\n",
    "y = data.target  # labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Encode the labels (One-hot encoding for multi-class)\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(10, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer (for multi-class classification)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy',  # For multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model and save the history\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 1: Visualize the accuracy curve\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)  # First subplot for accuracy\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Step 2: Visualize the loss curve\n",
    "plt.subplot(1, 2, 2)  # Second subplot for loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3102bc1-f81e-42d2-bae6-d0a49d0cd8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 174ms/step - accuracy: 0.3487 - loss: 1.3099 - val_accuracy: 0.2667 - val_loss: 1.3568\n",
      "Epoch 2/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3281 - loss: 1.2728 - val_accuracy: 0.2667 - val_loss: 1.3285\n",
      "Epoch 3/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.3635 - loss: 1.2222 - val_accuracy: 0.2667 - val_loss: 1.3018\n",
      "Epoch 4/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3323 - loss: 1.2373 - val_accuracy: 0.2667 - val_loss: 1.2756\n",
      "Epoch 5/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.2979 - loss: 1.2299 - val_accuracy: 0.2667 - val_loss: 1.2502\n",
      "Epoch 6/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3552 - loss: 1.1860 - val_accuracy: 0.2667 - val_loss: 1.2272\n",
      "Epoch 7/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3700 - loss: 1.1791 - val_accuracy: 0.2667 - val_loss: 1.2052\n",
      "Epoch 8/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3033 - loss: 1.1620 - val_accuracy: 0.2667 - val_loss: 1.1854\n",
      "Epoch 9/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3233 - loss: 1.1703 - val_accuracy: 0.3333 - val_loss: 1.1660\n",
      "Epoch 10/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3475 - loss: 1.1359 - val_accuracy: 0.3333 - val_loss: 1.1485\n",
      "Epoch 11/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.3652 - loss: 1.1225 - val_accuracy: 0.4000 - val_loss: 1.1312\n",
      "Epoch 12/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3613 - loss: 1.1149 - val_accuracy: 0.4333 - val_loss: 1.1143\n",
      "Epoch 13/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3773 - loss: 1.0856 - val_accuracy: 0.4333 - val_loss: 1.0977\n",
      "Epoch 14/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3892 - loss: 1.0903 - val_accuracy: 0.4667 - val_loss: 1.0802\n",
      "Epoch 15/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4365 - loss: 1.0533 - val_accuracy: 0.4667 - val_loss: 1.0637\n",
      "Epoch 16/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5125 - loss: 1.0531 - val_accuracy: 0.5667 - val_loss: 1.0462\n",
      "Epoch 17/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4594 - loss: 1.0462 - val_accuracy: 0.5667 - val_loss: 1.0291\n",
      "Epoch 18/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.5183 - loss: 1.0201 - val_accuracy: 0.6000 - val_loss: 1.0128\n",
      "Epoch 19/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5494 - loss: 1.0073 - val_accuracy: 0.6667 - val_loss: 0.9964\n",
      "Epoch 20/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6242 - loss: 0.9938 - val_accuracy: 0.6333 - val_loss: 0.9795\n",
      "Epoch 21/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.6425 - loss: 0.9786 - val_accuracy: 0.6333 - val_loss: 0.9631\n",
      "Epoch 22/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6490 - loss: 0.9695 - val_accuracy: 0.6667 - val_loss: 0.9463\n",
      "Epoch 23/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6081 - loss: 0.9513 - val_accuracy: 0.7333 - val_loss: 0.9295\n",
      "Epoch 24/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6210 - loss: 0.9442 - val_accuracy: 0.7333 - val_loss: 0.9130\n",
      "Epoch 25/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.7044 - loss: 0.9205 - val_accuracy: 0.8333 - val_loss: 0.8964\n",
      "Epoch 26/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.7158 - loss: 0.9139 - val_accuracy: 0.8333 - val_loss: 0.8797\n",
      "Epoch 27/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8025 - loss: 0.9062 - val_accuracy: 0.9000 - val_loss: 0.8625\n",
      "Epoch 28/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7996 - loss: 0.8919 - val_accuracy: 0.9000 - val_loss: 0.8452\n",
      "Epoch 29/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8110 - loss: 0.8645 - val_accuracy: 0.9000 - val_loss: 0.8274\n",
      "Epoch 30/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8394 - loss: 0.8443 - val_accuracy: 0.9333 - val_loss: 0.8092\n",
      "Epoch 31/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.8460 - loss: 0.8219 - val_accuracy: 0.9333 - val_loss: 0.7906\n",
      "Epoch 32/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8177 - loss: 0.8225 - val_accuracy: 0.9333 - val_loss: 0.7720\n",
      "Epoch 33/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8365 - loss: 0.7995 - val_accuracy: 0.9333 - val_loss: 0.7529\n",
      "Epoch 34/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.8408 - loss: 0.7980 - val_accuracy: 0.9333 - val_loss: 0.7342\n",
      "Epoch 35/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8106 - loss: 0.7834 - val_accuracy: 0.9333 - val_loss: 0.7152\n",
      "Epoch 36/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8733 - loss: 0.7366 - val_accuracy: 0.9000 - val_loss: 0.6952\n",
      "Epoch 37/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8240 - loss: 0.7320 - val_accuracy: 0.9000 - val_loss: 0.6755\n",
      "Epoch 38/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8502 - loss: 0.7221 - val_accuracy: 0.9000 - val_loss: 0.6559\n",
      "Epoch 39/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8535 - loss: 0.6944 - val_accuracy: 0.9000 - val_loss: 0.6366\n",
      "Epoch 40/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.8681 - loss: 0.6566 - val_accuracy: 0.9000 - val_loss: 0.6171\n",
      "Epoch 41/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.8702 - loss: 0.6435 - val_accuracy: 0.8667 - val_loss: 0.5978\n",
      "Epoch 42/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8296 - loss: 0.6676 - val_accuracy: 0.9000 - val_loss: 0.5789\n",
      "Epoch 43/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8642 - loss: 0.6177 - val_accuracy: 0.9000 - val_loss: 0.5608\n",
      "Epoch 44/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8717 - loss: 0.6033 - val_accuracy: 0.9000 - val_loss: 0.5431\n",
      "Epoch 45/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8477 - loss: 0.6129 - val_accuracy: 0.9000 - val_loss: 0.5264\n",
      "Epoch 46/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8581 - loss: 0.5748 - val_accuracy: 0.9000 - val_loss: 0.5098\n",
      "Epoch 47/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8675 - loss: 0.5620 - val_accuracy: 0.9000 - val_loss: 0.4934\n",
      "Epoch 48/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8612 - loss: 0.5766 - val_accuracy: 0.9000 - val_loss: 0.4771\n",
      "Epoch 49/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8758 - loss: 0.5380 - val_accuracy: 0.9000 - val_loss: 0.4606\n",
      "Epoch 50/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.8581 - loss: 0.5476 - val_accuracy: 0.9000 - val_loss: 0.4451\n",
      "Epoch 51/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.8454 - loss: 0.4998 - val_accuracy: 0.9000 - val_loss: 0.4305\n",
      "Epoch 52/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8798 - loss: 0.4810 - val_accuracy: 0.9000 - val_loss: 0.4168\n",
      "Epoch 53/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8673 - loss: 0.4968 - val_accuracy: 0.9000 - val_loss: 0.4040\n",
      "Epoch 54/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.8621 - loss: 0.4691 - val_accuracy: 0.9000 - val_loss: 0.3919\n",
      "Epoch 55/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8465 - loss: 0.4814 - val_accuracy: 0.9000 - val_loss: 0.3806\n",
      "Epoch 56/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8956 - loss: 0.4326 - val_accuracy: 0.9000 - val_loss: 0.3700\n",
      "Epoch 57/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8592 - loss: 0.4471 - val_accuracy: 0.9000 - val_loss: 0.3600\n",
      "Epoch 58/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.8687 - loss: 0.4290 - val_accuracy: 0.9000 - val_loss: 0.3506\n",
      "Epoch 59/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8719 - loss: 0.4200 - val_accuracy: 0.9000 - val_loss: 0.3418\n",
      "Epoch 60/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9075 - loss: 0.3910 - val_accuracy: 0.9000 - val_loss: 0.3333\n",
      "Epoch 61/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.8981 - loss: 0.3682 - val_accuracy: 0.9000 - val_loss: 0.3254\n",
      "Epoch 62/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8846 - loss: 0.3897 - val_accuracy: 0.9000 - val_loss: 0.3177\n",
      "Epoch 63/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8658 - loss: 0.4067 - val_accuracy: 0.9000 - val_loss: 0.3103\n",
      "Epoch 64/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8992 - loss: 0.3498 - val_accuracy: 0.9000 - val_loss: 0.3029\n",
      "Epoch 65/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8742 - loss: 0.3839 - val_accuracy: 0.9000 - val_loss: 0.2961\n",
      "Epoch 66/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8835 - loss: 0.3570 - val_accuracy: 0.9000 - val_loss: 0.2896\n",
      "Epoch 67/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.8752 - loss: 0.3515 - val_accuracy: 0.9000 - val_loss: 0.2835\n",
      "Epoch 68/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8710 - loss: 0.3576 - val_accuracy: 0.9000 - val_loss: 0.2774\n",
      "Epoch 69/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8971 - loss: 0.3447 - val_accuracy: 0.9000 - val_loss: 0.2715\n",
      "Epoch 70/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.8690 - loss: 0.3541 - val_accuracy: 0.9000 - val_loss: 0.2661\n",
      "Epoch 71/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.8754 - loss: 0.3593 - val_accuracy: 0.9000 - val_loss: 0.2608\n",
      "Epoch 72/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9087 - loss: 0.3146 - val_accuracy: 0.9333 - val_loss: 0.2555\n",
      "Epoch 73/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.8721 - loss: 0.3343 - val_accuracy: 0.9333 - val_loss: 0.2507\n",
      "Epoch 74/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8979 - loss: 0.3090 - val_accuracy: 0.9667 - val_loss: 0.2458\n",
      "Epoch 75/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8333 - loss: 0.3429 - val_accuracy: 0.9667 - val_loss: 0.2412\n",
      "Epoch 76/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9056 - loss: 0.2861 - val_accuracy: 0.9667 - val_loss: 0.2366\n",
      "Epoch 77/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9004 - loss: 0.2737 - val_accuracy: 0.9333 - val_loss: 0.2323\n",
      "Epoch 78/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8921 - loss: 0.2905 - val_accuracy: 0.9333 - val_loss: 0.2277\n",
      "Epoch 79/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.8840 - loss: 0.2943 - val_accuracy: 0.9667 - val_loss: 0.2235\n",
      "Epoch 80/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9004 - loss: 0.2918 - val_accuracy: 0.9333 - val_loss: 0.2192\n",
      "Epoch 81/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - accuracy: 0.8785 - loss: 0.2783 - val_accuracy: 0.9667 - val_loss: 0.2146\n",
      "Epoch 82/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9050 - loss: 0.2596 - val_accuracy: 0.9667 - val_loss: 0.2104\n",
      "Epoch 83/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9363 - loss: 0.2562 - val_accuracy: 0.9667 - val_loss: 0.2062\n",
      "Epoch 84/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.9123 - loss: 0.2689 - val_accuracy: 0.9667 - val_loss: 0.2021\n",
      "Epoch 85/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.9175 - loss: 0.2482 - val_accuracy: 0.9667 - val_loss: 0.1981\n",
      "Epoch 86/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9073 - loss: 0.2666 - val_accuracy: 0.9667 - val_loss: 0.1937\n",
      "Epoch 87/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8748 - loss: 0.2678 - val_accuracy: 0.9667 - val_loss: 0.1897\n",
      "Epoch 88/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.9283 - loss: 0.2452 - val_accuracy: 0.9667 - val_loss: 0.1853\n",
      "Epoch 89/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.9108 - loss: 0.2667 - val_accuracy: 0.9667 - val_loss: 0.1812\n",
      "Epoch 90/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9477 - loss: 0.2465 - val_accuracy: 0.9667 - val_loss: 0.1771\n",
      "Epoch 91/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.9717 - loss: 0.1995 - val_accuracy: 0.9667 - val_loss: 0.1734\n",
      "Epoch 92/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9540 - loss: 0.2388 - val_accuracy: 0.9667 - val_loss: 0.1697\n",
      "Epoch 93/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.9456 - loss: 0.2381 - val_accuracy: 0.9667 - val_loss: 0.1664\n",
      "Epoch 94/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9571 - loss: 0.2108 - val_accuracy: 0.9667 - val_loss: 0.1635\n",
      "Epoch 95/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9488 - loss: 0.2171 - val_accuracy: 0.9667 - val_loss: 0.1605\n",
      "Epoch 96/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.9698 - loss: 0.1980 - val_accuracy: 0.9667 - val_loss: 0.1573\n",
      "Epoch 97/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.9563 - loss: 0.2160 - val_accuracy: 1.0000 - val_loss: 0.1542\n",
      "Epoch 98/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.9448 - loss: 0.2199 - val_accuracy: 1.0000 - val_loss: 0.1513\n",
      "Epoch 99/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.9540 - loss: 0.1970 - val_accuracy: 1.0000 - val_loss: 0.1483\n",
      "Epoch 100/100\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.9508 - loss: 0.2015 - val_accuracy: 1.0000 - val_loss: 0.1454\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 1.0000 - loss: 0.1454\n",
      "Test Loss: 0.14536228775978088, Test Accuracy: 1.0\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001ACA746B6A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
      "Predicted labels: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "#9.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # features\n",
    "y = data.target  # labels\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Encode the labels (One-hot encoding for multi-class)\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "# Initialize the model\n",
    "model = Sequential()\n",
    "\n",
    "# Add layers\n",
    "model.add(Dense(10, input_dim=4, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(10, activation='relu'))  # Hidden layer\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer (for multi-class classification)\n",
    "\n",
    "# Compile the model with gradient clipping (clip gradients by value or norm)\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=1.0)  # Clip gradients by value (if they exceed 1.0)\n",
    "\n",
    "# Alternatively, you can use clipnorm to clip by gradient norm\n",
    "# optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)  # Clip gradients by norm (if norm exceeds 1.0)\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='sparse_categorical_crossentropy',  # For multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
    "\n",
    "# Predict class labels for the test data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a74215d8-ebf4-45dd-84be-1d8d1a1be5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.\n",
    "import tensorflow as tf\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # Calculate Mean Squared Error (MSE)\n",
    "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    \n",
    "    # Add a custom factor to the loss\n",
    "    custom_factor = 0.5\n",
    "    return mse + custom_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c614c33f-9b73-413b-8056-4dee1d5978ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,817</span> (11.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,817\u001b[0m (11.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,817</span> (11.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,817\u001b[0m (11.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#11\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a simple model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(10,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cfc1b9-771a-4bcd-acdc-63db0be5f36d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bdc1f3-165a-4e65-8af3-d6fbad3e0dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077f498-bc25-4c3e-a618-a858d79ac9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6be3b5-e818-4c9c-8af1-96df2601cfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
